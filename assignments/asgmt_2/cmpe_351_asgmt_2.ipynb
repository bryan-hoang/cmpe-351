{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPE 351 Assignment 2\n",
    "\n",
    "We'll first import the necessary python packages to run the code in the\n",
    "notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages.\n",
    "from os.path import dirname, join, realpath\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    \"\"\"Determine if the current session is interactive.\"\"\"\n",
    "    import __main__ as main\n",
    "\n",
    "    return not hasattr(main, \"__file__\")\n",
    "\n",
    "\n",
    "if is_interactive():\n",
    "    SCRIPT_DIR = dirname(realpath(\"__file__\"))\n",
    "else:\n",
    "    SCRIPT_DIR = dirname(realpath(__file__))\n",
    "\n",
    "DATA_DIR = join(SCRIPT_DIR, \"data\")\n",
    "IMG_DIR = join(DATA_DIR, \"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Image Classification using CNN (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "EPOCH_COUNT = 2\n",
    "CLASSES_COUNT = 13\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionProductImageDataset(VisionDataset):\n",
    "    \"\"\"Fashion product images dataset.\"\"\"\n",
    "\n",
    "    classes = [\n",
    "        \"Topwear\",\n",
    "        \"Bottomwear\",\n",
    "        \"Innerwear\",\n",
    "        \"Bags\",\n",
    "        \"Watches\",\n",
    "        \"Jewellery\",\n",
    "        \"Eyewear\",\n",
    "        \"Wallets\",\n",
    "        \"Shoes\",\n",
    "        \"Sandal\",\n",
    "        \"Makeup\",\n",
    "        \"Fragrance\",\n",
    "        \"Others\",\n",
    "    ]\n",
    "\n",
    "    target_encoder = LabelEncoder()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Callable = None,\n",
    "        target_transform: Callable = None,\n",
    "        targets_file: str = None,\n",
    "    ):\n",
    "        \"\"\"Construct FashionProductImageDataset.\n",
    "\n",
    "        Args:\n",
    "            root (string): Root directory of dataset where directory\n",
    "                ``cifar-10-batches-py`` exists or will be saved to if download\n",
    "                is set to True.\n",
    "            transform (callable, optional): A function/transform that takes in\n",
    "                an PIL image and returns a transformed version.\n",
    "                E.g, ``transforms.RandomCrop``\n",
    "            target_transform (callable, optional): A function/transform that\n",
    "                takes in the target and transforms it.\n",
    "            targets_file (string): Path to the csv file with annotations.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        fashion_products_data_frame = pd.read_csv(targets_file, sep=\"\\t\")\n",
    "\n",
    "        self.img_ids = fashion_products_data_frame[:][\"imageid\"]\n",
    "\n",
    "        self.target_encoder.fit(FashionProductImageDataset.classes)\n",
    "        self.targets = self.target_encoder.transform(\n",
    "            fashion_products_data_frame[\"label\"]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset.\"\"\"\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"Get the image and target for the given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = (\n",
    "            Image.open(\n",
    "                join(self.root, f\"{self.img_ids.iloc[index]}.jpg\")\n",
    "            ).convert(\"RGB\"),\n",
    "            self.targets[index],\n",
    "        )\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionProductImageDataset(\n",
    "    join(DATA_DIR, \"img\"),\n",
    "    transform=img_transform,\n",
    "    targets_file=join(DATA_DIR, \"train.csv\"),\n",
    ")\n",
    "\n",
    "test_dataset = FashionProductImageDataset(\n",
    "    join(DATA_DIR, \"img\"),\n",
    "    transform=img_transform,\n",
    "    targets_file=join(DATA_DIR, \"test.csv\"),\n",
    ")\n",
    "\n",
    "# Create loader objects to facilitate processing\n",
    "train_loader: DataLoader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "test_loader: DataLoader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Check number of train/test data\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 6, 28, 28])\n",
      "torch.Size([32, 6, 14, 14])\n",
      "torch.Size([32, 16, 10, 10])\n",
      "torch.Size([32, 16, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "def trial_layers(train_loader):\n",
    "    \"\"\"Test the shape of the layers for a CNN.\"\"\"\n",
    "    dataiter = iter(train_loader)\n",
    "    (images, _) = dataiter.next()\n",
    "\n",
    "    conv1 = nn.Conv2d(3, 6, 5)\n",
    "    pool = nn.MaxPool2d(2, 2)\n",
    "    conv2 = nn.Conv2d(6, 16, 5)\n",
    "    print(images.shape)\n",
    "    trials = conv1(images)\n",
    "    print(trials.shape)\n",
    "    trials = pool(trials)\n",
    "    print(trials.shape)\n",
    "    trials = conv2(trials)\n",
    "    print(trials.shape)\n",
    "    trials = pool(trials)\n",
    "    print(trials.shape)\n",
    "\n",
    "\n",
    "trial_layers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN class\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"Determine what layers and their order in CNN object.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Progresses data across layers.\"\"\"\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(CLASSES_COUNT)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=0.005, momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch_count, train_loader, model, criterion, optimizer):\n",
    "    \"\"\"Train a DL model.\"\"\"\n",
    "    n_total_steps = len(train_loader)\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        # Load in the data in batches using the train_loader object\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(\n",
    "                    f\"epoch {epoch+1}/{epoch_count},\"\n",
    "                    f\" step {i+1}/{n_total_steps}, loss = {loss.item():4f}\"\n",
    "                )\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epoch_count}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "41d23ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(train_loader, test_loader, model):\n",
    "    \"\"\"Evaluate a DL model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Accuracy of the network on the {len(test_loader)} test images:\"\n",
    "            f\" {100 * correct / total} %\"\n",
    "        )\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Accuracy of the network on the {len(train_loader)} train images:\"\n",
    "            f\" {100 * correct / total} %\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 500/1264, loss = 1.635015\n",
      "epoch 1/2, step 1000/1264, loss = 1.256636\n",
      "Epoch [1/2], Loss: 1.1399\n",
      "epoch 2/2, step 500/1264, loss = 1.120829\n",
      "epoch 2/2, step 1000/1264, loss = 0.935181\n",
      "Epoch [2/2], Loss: 1.1313\n"
     ]
    }
   ],
   "source": [
    "train_model(EPOCH_COUNT, train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 125 test images: 75.1 %\n",
      "Accuracy of the network on the 1264 train images: 74.9668099277694 %\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(train_loader, test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Improved Image Classification (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "EPOCH_COUNT = 4\n",
    "CLASSES_COUNT = 13\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 6, 28, 28])\n",
      "torch.Size([32, 6, 15, 15])\n",
      "torch.Size([32, 16, 11, 11])\n",
      "torch.Size([32, 16, 6, 6])\n",
      "torch.Size([32, 32, 2, 2])\n",
      "torch.Size([32, 32, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "def trial_layers_v2(train_loader):\n",
    "    \"\"\"Test the shape of the layers for a CNN.\"\"\"\n",
    "    dataiter = iter(train_loader)\n",
    "    (images, _) = dataiter.next()\n",
    "\n",
    "    conv1 = nn.Conv2d(3, 6, 5)\n",
    "    pool = nn.MaxPool2d(2, 2, 1)\n",
    "    conv2 = nn.Conv2d(6, 16, 5)\n",
    "    conv3 = nn.Conv2d(16, 32, 5)\n",
    "    print(images.shape)\n",
    "    trials = conv1(images)\n",
    "    print(trials.shape)\n",
    "    trials = pool(trials)\n",
    "    print(trials.shape)\n",
    "    trials = conv2(trials)\n",
    "    print(trials.shape)\n",
    "    trials = pool(trials)\n",
    "    print(trials.shape)\n",
    "    trials = conv3(trials)\n",
    "    print(trials.shape)\n",
    "    trials = pool(trials)\n",
    "    print(trials.shape)\n",
    "\n",
    "\n",
    "trial_layers_v2(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "208bddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetworkV2(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"Determine what layers and their order in CNN object.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 2 * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Progresses data across layers.\"\"\"\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "996f8674",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/4, step 500/1264, loss = 1.240248\n",
      "epoch 1/4, step 1000/1264, loss = 0.999678\n",
      "Epoch [1/4], Loss: 1.1389\n",
      "epoch 2/4, step 500/1264, loss = 1.119123\n",
      "epoch 2/4, step 1000/1264, loss = 0.818494\n",
      "Epoch [2/4], Loss: 0.3830\n",
      "epoch 3/4, step 500/1264, loss = 0.848047\n",
      "epoch 3/4, step 1000/1264, loss = 0.339255\n",
      "Epoch [3/4], Loss: 0.5753\n",
      "epoch 4/4, step 500/1264, loss = 0.762600\n",
      "epoch 4/4, step 1000/1264, loss = 0.474538\n",
      "Epoch [4/4], Loss: 0.7833\n"
     ]
    }
   ],
   "source": [
    "model_v2 = ConvolutionalNeuralNetworkV2(CLASSES_COUNT)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model_v2.parameters(), lr=LEARNING_RATE, weight_decay=0.005, momentum=0.9\n",
    ")\n",
    "\n",
    "train_model(EPOCH_COUNT, train_loader, model_v2, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 125 test images: 81.625 %\n",
      "Accuracy of the network on the 1264 train images: 81.6543282104363 %\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(train_loader, test_loader, model_v2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d1d4a31fd394edaf21d74294ba63853de24e6c25ad80bd4e86b4df595896c02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('cmpe-351-_rWzjxJw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
